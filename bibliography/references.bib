% Tokenization approaches and algorithms

@inproceedings{kudo2018sentencepiece,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={66--71},
  year={2018}
}


@inproceedings{bostrom2020byte,
  title={Byte Pair Encoding is Suboptimal for Language Model Pretraining},
  author={Bostrom, Kaj and Durrett, Greg},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4617--4624},
  year={2020}
}

@article{gage1994new,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={The C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={R \& D Publications, Inc.}
}

% Language models and transformer architectures
@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

% Domain-specific tokenization and NLP
@inproceedings{nguyen2020bertweet,
  title={BERTweet: A pre-trained language model for English Tweets},
  author={Nguyen, Dat Quoc and Vu, Thanh and Tuan Nguyen, Anh},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={9--14},
  year={2020}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@inproceedings{beltagy2019scibert,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3615--3620},
  year={2019}
}

% Legal and financial NLP
@inproceedings{chalkidis2020legal,
  title={Legal-BERT: The Muppets straight out of Law School},
  author={Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={2898--2904},
  year={2020}
}

@inproceedings{zheng2021does,
  title={Does BERToid need a specialized tokenizer: The case study of legal domain},
  author={Zheng, Lei and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Dean E},
  booktitle={Workshop on Natural Legal Language Processing},
  year={2021}
}

@inproceedings{tsarapatsanis2021ethical,
  title={On the Ethical Limits of Natural Language Processing on Legal Text},
  author={Tsarapatsanis, Dimitrios and Aletras, Nikolaos},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5213--5227},
  year={2021}
}

@article{araci2019finbert,
  title={FinBERT: Financial Sentiment Analysis with Pre-trained Language Models},
  author={Araci, Dogu},
  journal={arXiv preprint arXiv:1908.10063},
  year={2019}
}

@inproceedings{mansar2021finsim,
  title={The finsim-2 2021 shared task: Learning semantic similarities for the financial domain},
  author={Mansar, Youness and Kang, Juyeon and Maarouf, Ismail El},
  booktitle={Companion Proceedings of the Web Conference 2021},
  pages={288--292},
  year={2021}
}


% Character-level approaches
@inproceedings{ma2020charbert,
  title={CharBERT: Character-aware Pre-trained Language Model},
  author={Ma, Wentao and Cui, Yiming and Si, Chengsong and Liu, Ting and Wang, Shijin and Hu, Guoping},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={39--50},
  year={2020}
}

@inproceedings{wang2022deepstructure,
  title={DeepStruct: Pretraining of Language Models for Structure Prediction},
  author={Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={803--823},
  year={2022}
}

@article{clark2022canine,
  title={Canine: Pre-training an efficient tokenization-free encoder for language representation},
  author={Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={73--91},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

% Tokenization challenges and evaluation
@inproceedings{webster2020measuring,
  title={Measuring and Reducing Gendered Correlations in Pre-trained Models},
  author={Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},
  booktitle={arXiv preprint arXiv:2010.06032},
  year={2020}
}

@inproceedings{tan2020its,
  title={It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations},
  author={Tan, Samson and Joty, Shafiq and Kan, Min-Yen and Socher, Richard},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2920--2935},
  year={2020}
}

@inproceedings{rust2020good,
  title={How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
  author={Rust, Phillip and Pfeiffer, Jonas and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3118--3135},
  year={2021}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

@inproceedings{webster1992tokenization,
  title={Tokenization as the initial phase in NLP},
  author={Webster, Jonathan J and Kit, Chunyu},
  booktitle={COLING 1992 volume 4: The 14th international conference on computational linguistics},
  year={1992}
}

@article{mielke2021between,
  title={Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  author={Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot, Beno{\^\i}t and others},
  journal={arXiv preprint arXiv:2112.10508},
  year={2021}
}